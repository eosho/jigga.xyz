# Argo Workflows Alerts
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: argo-workflows-alerts
  namespace: monitoring
  labels:
    app.kubernetes.io/name: argo-workflows
    app.kubernetes.io/component: alerts
    release: kube-prometheus-stack
spec:
  groups:
    - name: argo-workflows.rules
      rules:
        - alert: ArgoWorkflowsControllerNotLeader
          expr: argo_workflows_is_leader == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Argo Workflows controller has no leader"
            description: "Argo Workflows controller has had no leader for 5 minutes. Workflows cannot be processed."

        - alert: ArgoWorkflowsHasFailedWorkflows
          expr: argo_workflows_gauge{status="Failed"} > 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "There are {{ $value }} failed Argo Workflows"
            description: |
              Failed workflows exist and have not been cleaned up.

              To find failed workflows:
                kubectl get workflows -A --field-selector=status.phase=Failed

              Or check the Argo UI: https://workflows.int.jigga.xyz/workflows?phase=Failed
            runbook_url: "https://workflows.int.jigga.xyz/workflows?phase=Failed"

        - alert: ArgoWorkflowsHasErroredWorkflows
          expr: argo_workflows_gauge{status="Error"} > 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "There are {{ $value }} errored Argo Workflows"
            description: |
              Errored workflows exist.

              To find errored workflows:
                kubectl get workflows -A --field-selector=status.phase=Error

              Or check the Argo UI: https://workflows.int.jigga.xyz/workflows?phase=Error
            runbook_url: "https://workflows.int.jigga.xyz/workflows?phase=Error"

        - alert: ArgoWorkflowsPendingTooLong
          expr: argo_workflows_gauge{status="Pending"} > 0
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "Workflows pending for over 15 minutes"
            description: "{{ $value }} workflows have been pending for more than 15 minutes. Check for resource constraints."

        - alert: ArgoCronWorkflowSpecError
          expr: increase(argo_workflows_error_count{cause="CronWorkflowSpecError"}[1h]) > 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "CronWorkflow spec errors detected"
            description: "CronWorkflow spec errors occurred in the last hour. Check CronWorkflow definitions."

        - alert: ArgoCronWorkflowSubmissionError
          expr: increase(argo_workflows_error_count{cause="CronWorkflowSubmissionError"}[1h]) > 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "CronWorkflow submission errors detected"
            description: "CronWorkflow submission errors occurred in the last hour."

        - alert: ArgoWorkflowsQueueBacklog
          expr: argo_workflows_queue_depth_gauge > 50
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "Argo Workflows queue has large backlog"
            description: "Argo Workflows queue has {{ $value }} items waiting for more than 15 minutes."

        - alert: ArgoWorkflowsQueueStuck
          expr: argo_workflows_queue_longest_running > 300
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Argo Workflows queue item stuck"
            description: "An item has been in the queue for {{ $value }} seconds."

        - alert: ArgoWorkflowsPodsMissing
          expr: argo_workflows_pod_missing > 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Argo Workflows has missing pods"
            description: "{{ $value }} workflow pods are missing. This may indicate node issues or evictions."

        - alert: ArgoWorkflowsPodsPendingHigh
          expr: argo_workflows_pod_pending_count > 10
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Many workflow pods pending"
            description: "{{ $value }} workflow pods are pending. Check for resource constraints or scheduling issues."

    # CronWorkflow-specific alerts (these have name labels)
    - name: argo-cronworkflows.rules
      rules:
        - alert: PostgresBackupFailed
          # Alert if postgres backup hasn't succeeded in 25 hours (daily backup + 1hr buffer)
          expr: |
            (time() - argo_workflows_workflowtemplate_triggered_total{name="postgres-backup", phase="Succeeded"}) > 90000
            or
            absent(argo_workflows_workflowtemplate_triggered_total{name="postgres-backup", phase="Succeeded"})
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "PostgreSQL daily backup has not succeeded"
            description: |
              The postgres-backup workflow has not completed successfully in over 25 hours.

              Check recent backup workflows:
                kubectl get workflows -n argo-workflows -l workflows.argoproj.io/workflow-template=postgres-backup --sort-by=.metadata.creationTimestamp

              View logs in Loki:
                {namespace="argo-workflows", workflow_template="postgres-backup"}
            runbook_url: "https://workflows.int.jigga.xyz/workflow-templates/argo-workflows/postgres-backup"
